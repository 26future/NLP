{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM(Long Short Term Memory)\n",
    "https://dgkim5360.tistory.com/entry/understanding-long-short-term-memory-lstm-kr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ê¸°ìš¸ê¸° ì†Œì‹¤(Gradient Vanishing)ê³¼ ê¸°ìš¸ê¸° í­ë°œ(Gradient Exploding)ì˜ ì›ì¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dh:\n",
      " [[1. 1. 1.]\n",
      " [1. 1. 1.]]\n",
      "Wh:\n",
      " [[ 1.78862847  0.43650985  0.09649747]\n",
      " [-1.8634927  -0.2773882  -0.35475898]\n",
      " [-0.08274148 -0.62700068 -0.04381817]]\n",
      "[2.4684068094579303, 3.335704974161037, 4.783279375373183, 6.2795873320876145, 8.080776465019055, 10.25116303229294, 12.9360635066099, 16.276861327786712, 20.454829618345983, 25.688972842084684, 32.25315718048336, 40.48895641683869, 50.824407307019094, 63.79612654485427, 80.07737014308985, 100.51298922051251, 126.16331847536827, 158.3592064825883, 198.77107967611957, 249.495615421267]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rc('font', family='Malgun Gothic')\n",
    "\n",
    "N = 2  # ë¯¸ë‹ˆë°°ì¹˜ í¬ê¸°\n",
    "H = 3  # ì€ë‹‰ ìƒíƒœ ë²¡í„°ì˜ ì°¨ì› ìˆ˜\n",
    "T = 20 # ì‹œê³„ì—´ ë°ì´í„°ì˜ ê¸¸ì´\n",
    "\n",
    "dh = np.ones((N,H))  # (2,3)\n",
    "print('dh:\\n',dh)\n",
    "\n",
    "np.random.seed(3)   # ì¬í˜„í•  ìˆ˜ ìˆë„ë¡ ë‚œìˆ˜ì˜ ì‹œë“œ ê³ ì •\n",
    "\n",
    "Wh = np.random.randn(H,H)         # ê¸°ìš¸ê¸° í­ë°œ (ìŠ¤ì¹¼ë¼ì¼ ê²½ìš° Whê°€ 1ë³´ë‹¤ í° ê²½ìš°)   --> NaN, ë°œì‚°\n",
    "# Wh = np.random.randn(H,H)*0.5   # ê¸°ìš¸ê¸° ì†Œì‹¤ (ìŠ¤ì¹¼ë¼ì¼ ê²½ìš° Whê°€ 1ë³´ë‹¤ ì‘ì€ ê²½ìš°) --> 0 \n",
    "print('Wh:\\n',Wh)\n",
    "\n",
    "\n",
    "# Normì€ ë²¡í„°ì˜ ê¸¸ì´ í˜¹ì€ í¬ê¸°ë¥¼ ì¸¡ì •í•˜ëŠ” ë°©ë²•(í•¨ìˆ˜)ì´ë‹¤\n",
    "# L1 norm : ë²¡í„°ì˜ ê° ìš”ì†Œì˜ ì ˆëŒ€ê°’ì„ ëª¨ë‘ í•©í•œ ê°’\n",
    "# L2 norm : ê° ìš”ì†Œì˜ ì œê³±ì„ ëª¨ë‘ í•©í•˜ì—¬ ì œê³±ê·¼ì„ ì·¨í•œ ê°’\n",
    "# http://taewan.kim/post/norm/\n",
    "norm_list = []\n",
    "for t in range(T):\n",
    "    dh = np.dot(dh,Wh.T)   # ë¯¸ë¶„, gradient ê°’\n",
    "    # print('-'*30)\n",
    "    # print(dh)\n",
    "    norm = np.sqrt(np.sum(dh**2))/N  # í‰ê·  norm\n",
    "    # print(norm)\n",
    "    norm_list.append(norm)\n",
    "\n",
    "print(norm_list)  \n",
    "\n",
    "# ê·¸ë˜í”„ ê·¸ë¦¬ê¸°\n",
    "plt.plot(np.arange(len(norm_list)), norm_list)\n",
    "plt.xticks([0, 4, 9, 14, 19], [1, 5, 10, 15, 20])\n",
    "plt.xlabel('ì‹œê°„ í¬ê¸°(time step)')\n",
    "plt.ylabel('ë…¸ë¦„(norm)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ê¸°ìš¸ê¸° í­ë°œ ëŒ€ì±… : ê¸°ìš¸ê¸° í´ë¦¬í•‘(gradient cliping) í•¨ìˆ˜ êµ¬í˜„\n",
    "https://wikidocs.net/61375"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dW1:\n",
      " [[6.49144048 2.78487283 6.76254902]\n",
      " [5.90862817 0.23981882 5.58854088]\n",
      " [2.59252447 4.15101197 2.83525082]]\n",
      "dW2:\n",
      " [[6.93137918 4.40453718 1.56867738]\n",
      " [5.44649018 7.80314765 3.06363532]\n",
      " [2.21957884 3.87971258 9.3638365 ]]\n",
      "(dW1) before: [6.49144048 2.78487283 6.76254902 5.90862817 0.23981882 5.58854088\n",
      " 2.59252447 4.15101197 2.83525082]\n",
      "(dw1) after: [1.49503731 0.64138134 1.55747605 1.36081038 0.05523244 1.28709139\n",
      " 0.59708178 0.95601551 0.65298384]\n"
     ]
    }
   ],
   "source": [
    "dW1 = np.random.rand(3,3)*10\n",
    "dW2 = np.random.rand(3,3)*10\n",
    "print('dW1:\\n',dW1)\n",
    "print('dW2:\\n',dW2)\n",
    "\n",
    "grads = [dW1,dW2]\n",
    "\n",
    "max_norm = 5.0   # threshold, í•œê³„ê°’\n",
    "\n",
    "## nn_layers.py ì— ì¶”ê°€í•œë‹¤\n",
    "def clip_grads(grads, max_norm):\n",
    "    total_norm = 0\n",
    "    for grad in grads:  # L2 norm êµ¬í•˜ê¸° , ì œê³±ì˜ í•©ì˜ ì œê³±ê·¼\n",
    "        total_norm += np.sum(grad ** 2)\n",
    "    total_norm = np.sqrt(total_norm)\n",
    "\n",
    "    rate = max_norm / (total_norm + 1e-6)\n",
    "    # print('rate:',rate)\n",
    "    if rate < 1:     # total_norm ì´ í•œê³„ê°’(max_norm) ë³´ë‹¤ í´ê²½ìš°\n",
    "        for grad in grads:\n",
    "            grad *= rate\n",
    "    \n",
    "\n",
    "print('(dW1) before:', dW1.flatten())\n",
    "#print('(dW2) before:', dW2.flatten())\n",
    "clip_grads(grads, max_norm)\n",
    "print('(dw1) after:', dW1.flatten())  # ê°’ì´ ì•½ê°„ ì¤„ì–´ë“¦    \n",
    "#print('(dw2) after:', dW2.flatten())  # ê°’ì´ ì•½ê°„ ì¤„ì–´ë“¦"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ê¸°ìš¸ê¸° ì†Œì‹¤ ë°©ì§€ : Gated RNN ì¸ LSTM(Long Short Term Memory)ì´ë‚˜  GRU(Gated Recurrent Units ) ì‚¬ìš©"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM  : RNNì— ê¸°ì–µ ì…€ ğœ<sub>ğ­</sub> ê³¼ , f , g, i, o ê²Œì´íŠ¸ ì¶”ê°€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "# nn_layer.py ì— sigmoid í•¨ìˆ˜ë¥¼ ì¶”ê°€í•œë‹¤\n",
    "\n",
    "class LSTM:\n",
    "    def __init__(self, Wx, Wh, b):\n",
    "        '''\n",
    "        Parameters\n",
    "        ----------\n",
    "        Wx: ì…ë ¥ xì— ëŒ€í•œ ê°€ì¤‘ì¹˜ ë§¤ê°œë³€ìˆ˜(4ê°œë¶„ì˜ ê°€ì¤‘ì¹˜ê°€ ë‹´ê²¨ ìˆìŒ)\n",
    "        Wh: ì€ë‹‰ ìƒíƒœ hì— ëŒ€í•œ ê°€ì¤‘ì¹˜ ë§¤ê°œë³€ìˆ˜(4ê°œë¶„ì˜ ê°€ì¤‘ì¹˜ê°€ ë‹´ê²¨ ìˆìŒ)\n",
    "        b: í¸í–¥ï¼ˆ4ê°œë¶„ì˜ í¸í–¥ì´ ë‹´ê²¨ ìˆìŒï¼‰\n",
    "        '''\n",
    "        self.params = [Wx,Wh,b]\n",
    "        self.grads = [np.zeros_like(Wx), np.zeros_like(Wh), np.zeros_like(b)]\n",
    "        self.cache = None\n",
    "        \n",
    "\n",
    "    def forward(self,x,h_prev,c_prev):\n",
    "        Wx,Wh,b = self.params            # Wx,Wh : (D,4*H)ë¡œ ìƒì„±í•˜ì—¬ ì…ë ¥ë¨\n",
    "        N,H = h_prev.shape\n",
    "        \n",
    "        A = np.dot(x,Wx) + np.dot(h_prev,Wh) + b\n",
    "        \n",
    "        # ë™ì¼í•œ ì‚¬ì´ì¦ˆë¡œ 4ê°œë¡œ ìŠ¬ë¼ì´ì‹± : f,g,i,o\n",
    "        f = A[:, :H]\n",
    "        g = A[:, H:2*H]\n",
    "        i = A[:, 2*H:3*H]\n",
    "        o = A[:, 3*H:]\n",
    "        \n",
    "        f = sigmoid(f)\n",
    "        g = np.tanh(g)\n",
    "        i = sigmoid(i)\n",
    "        o = sigmoid(o)\n",
    "        \n",
    "        c_next = f*c_prev + g*i\n",
    "        h_next = o*np.tanh(c_next)\n",
    "        \n",
    "        self.cache = (x, h_prev, c_prev, c_next, f, g, i, o)\n",
    "        \n",
    "        return h_next, c_next\n",
    "    \n",
    "    def backward(self,dh_next,dc_next):\n",
    "        Wx,Wh,b = self.params\n",
    "        x,h_prev,c_prev,c_next,f,g,i,o = self.cache\n",
    "        \n",
    "        tanh_c_next = np.tanh(c_next)\n",
    "        \n",
    "        ds = dc_next + (dh_next * o)*(1 - tanh_c_next**2)\n",
    "        \n",
    "        dc_prev = ds.f\n",
    "        \n",
    "        di = ds * g\n",
    "        df = ds * c_prev\n",
    "        dg = ds * i\n",
    "        do = dh_next * tanh_c_next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
